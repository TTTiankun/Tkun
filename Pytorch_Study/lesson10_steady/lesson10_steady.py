#数值的稳定性

#神经网络的梯度
#梯度爆炸 梯度消失
#梯度爆炸：梯度过大，导致权重更新过大，网络不稳定
#梯度消失：梯度过小，导致权重更新过小，网络不稳定

#例子MLP 
#激活函数：sigmoid tanh
#梯度函数：激活函数的导数
#梯度消失：sigmoid函数在0附近梯度接近0，导致梯度消失
#梯度爆炸：sigmoid函数在0附近梯度接近1，导致梯度爆炸

#梯度爆炸
#使用RELU函数作为激活函数

#梯度爆炸的问题
#1.梯度爆炸会导致网络不稳定，难以收敛
#2.梯度爆炸会导致梯度溢出，权重更新过大，网络发散 这个对16位浮点数影响很大
#3.对学习率特别的敏感
#  如果学习率太大，大参数值，更大的梯度
#  如果学习率太小，训练无进展
#  可能需要再训练过程之中不断地调整学习率

#梯度消失的问题
#训练不会有进展 权重就是梯度乘以学习率
#梯度消失，权重更新很小，网络不稳定
#仅仅有顶部层训练的比较好，但是深度神经网络没有什么改变
